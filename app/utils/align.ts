import { z } from 'zod'
import type { Sentence, WordWithTime } from '~/types'
import { chatGPT, deepSeek, gemini } from './ai'

type SplitTextToSentencesOptions = {
	text: string
	maxSentenceLength?: number
}

// å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­,ä½¿ç”¨å¥å­ç»“æŸç¬¦å·åˆ†å‰²ï¼Œä½†æ˜¯éœ€è¦è€ƒè™‘ä¸€äº›ç‰¹æ®Šæƒ…å†µä¸åˆ†å‰²ï¼Œæ¯”å¦‚å°æ•°ç‚¹ï¼Œç§‘å­¦è®¡æ•°ï¼Œä¸€äº›ç‰¹æ®Šçš„ç¼©å†™ï¼Œæ¯”å¦‚['U.S','A.I']
// å¦‚æœå¥å­é•¿åº¦è¶…è¿‡maxSentenceLengthï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨å…¶ä»–æ ‡ç‚¹åˆ†å‰²ï¼Œå¦‚æœæ²¡æœ‰å…¶ä»–æ ‡ç‚¹ï¼Œå°±ä¸åˆ†å‰²
export function splitTextToSentences({ text, maxSentenceLength = 100 }: SplitTextToSentencesOptions): string[] {
	if (!text) return []

	// Common abbreviations that contain periods but shouldn't split sentences
	const commonAbbreviations = ['U.S', 'A.I', 'Mr.', 'Mrs.', 'Dr.', 'Ph.D', 'e.g.', 'i.e.', 'etc.']

	// Replace abbreviations with temporary placeholders to avoid splitting at their periods
	let processedText = text
	const abbreviationMap = new Map<string, string>()

	commonAbbreviations.forEach((abbr, index) => {
		const placeholder = `__ABBR${index}__`
		const regex = new RegExp(abbr.replace(/\./g, '\\.'), 'g')
		processedText = processedText.replace(regex, placeholder)
		abbreviationMap.set(placeholder, abbr)
	})

	// Regular expression to match sentence endings while ignoring decimal points and scientific notation
	// This regex looks for:
	// - A period, question mark, or exclamation mark
	// - Followed by a space or end of string
	// - But not preceded by a digit if followed by a digit (to avoid splitting decimal points)
	// - And not preceded by 'e' or 'E' if followed by '+' or '-' (to avoid splitting scientific notation)
	const sentenceEndRegex = /(?<![0-9])(?<![eE])[.!?](?=\s|$)/g

	// Split by sentence endings
	const sentences = processedText
		.split(sentenceEndRegex)
		.map((s) => s.trim())
		.filter(Boolean)

	// If any sentence is too long, try to split it further using other punctuation
	const result: string[] = []

	for (const sentence of sentences) {
		if (sentence.length <= maxSentenceLength) {
			result.push(sentence)
		} else {
			// Try to split by commas, semicolons, or colons
			const secondaryPunctuation = /[,;:](?=\s)/g
			const parts = sentence
				.split(secondaryPunctuation)
				.map((p) => p.trim())
				.filter(Boolean)

			let currentPart = ''

			for (const part of parts) {
				if ((currentPart + part).length > maxSentenceLength && currentPart) {
					result.push(currentPart)
					currentPart = part
				} else {
					currentPart = currentPart ? `${currentPart} ${part}` : part
				}
			}

			if (currentPart) {
				result.push(currentPart)
			}
		}
	}

	// Restore abbreviations
	return result.map((sentence) => {
		let restored = sentence
		abbreviationMap.forEach((original, placeholder) => {
			restored = restored.replace(new RegExp(placeholder, 'g'), original)
		})
		return restored
	})
}

export function alignWordsAndSentences(words: WordWithTime[], sentences: string[]): Sentence[] {
	if (!words.length || !sentences.length) {
		return []
	}

	// æ„å»ºå®Œæ•´çš„è½¬å½•æ–‡æœ¬
	const fullTranscript = words.map((w) => w.word).join('')

	// è§„èŒƒåŒ–æ–‡æœ¬ä»¥ä¾¿äºåŒ¹é…
	const normalizeText = (text: string) => {
		return text.replace(/\s+/g, ' ').trim().toLowerCase()
	}

	// åˆ›å»ºç»“æœæ•°ç»„
	const result: Sentence[] = []

	// åˆ›å»ºå•è¯ç´¢å¼•æ˜ å°„ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾å•è¯ä½ç½®
	const wordPositions: { [key: string]: number[] } = {}
	let currentPos = 0
	words.forEach((word, index) => {
		const normalizedWord = normalizeText(word.word)
		if (!wordPositions[normalizedWord]) {
			wordPositions[normalizedWord] = []
		}
		wordPositions[normalizedWord].push(index)
		currentPos += word.word.length
	})

	// ç¬¬ä¸€é˜¶æ®µï¼šå°è¯•åŒ¹é…æ¯ä¸ªå¥å­çš„å¼€å¤´å’Œç»“å°¾
	const normalizedTranscript = normalizeText(fullTranscript)
	const sentenceData: {
		sentence: string
		normalizedSentence: string
		firstWords: string[]
		lastWords: string[]
		matchedStartIndex: number
		matchedEndIndex: number
		processed: boolean
	}[] = sentences.map((sentence) => {
		const normalizedSentence = normalizeText(sentence)
		const words = normalizedSentence.split(/\s+/).filter((w) => w.length > 0)

		return {
			sentence,
			normalizedSentence,
			firstWords: words.slice(0, Math.min(3, words.length)),
			lastWords: words.slice(Math.max(0, words.length - 3)),
			matchedStartIndex: -1,
			matchedEndIndex: -1,
			processed: false,
		}
	})

	// æ‰¾åˆ°å¯èƒ½çš„å¥å­è¾¹ç•Œ
	for (const sentenceInfo of sentenceData) {
		if (!sentenceInfo.sentence.trim()) {
			sentenceInfo.processed = true
			continue
		}

		// ç›´æ¥åŒ¹é…å®Œæ•´å¥å­
		const directMatchIndex = normalizedTranscript.indexOf(sentenceInfo.normalizedSentence)
		if (directMatchIndex !== -1) {
			// æ‰¾åˆ°å¯¹åº”çš„å•è¯ç´¢å¼•
			let charCount = 0
			let startWordIdx = -1
			let endWordIdx = -1

			for (let i = 0; i < words.length; i++) {
				const prevCharCount = charCount
				charCount += words[i].word.length

				if (prevCharCount <= directMatchIndex && charCount > directMatchIndex && startWordIdx === -1) {
					startWordIdx = i
				}

				if (startWordIdx !== -1 && charCount >= directMatchIndex + sentenceInfo.normalizedSentence.length) {
					endWordIdx = i
					break
				}
			}

			if (startWordIdx !== -1 && endWordIdx !== -1) {
				result.push({
					words: words.slice(startWordIdx, endWordIdx + 1),
					text: sentenceInfo.sentence,
					start: words[startWordIdx].start,
					end: words[endWordIdx].end,
				})
				sentenceInfo.processed = true
				continue
			}
		}

		// å°è¯•åŒ¹é…ç¬¬ä¸€ä¸ªå•è¯ç»„åˆ
		for (let numFirstWords = Math.min(3, sentenceInfo.firstWords.length); numFirstWords > 0; numFirstWords--) {
			const firstWordsPhrase = sentenceInfo.firstWords.slice(0, numFirstWords).join(' ')
			const firstPhraseIndex = normalizedTranscript.indexOf(firstWordsPhrase)

			if (firstPhraseIndex !== -1) {
				sentenceInfo.matchedStartIndex = firstPhraseIndex
				break
			}
		}

		// å°è¯•åŒ¹é…æœ€åä¸€ä¸ªå•è¯ç»„åˆ
		for (let numLastWords = Math.min(3, sentenceInfo.lastWords.length); numLastWords > 0; numLastWords--) {
			const lastWordsPhrase = sentenceInfo.lastWords.slice(sentenceInfo.lastWords.length - numLastWords).join(' ')
			// ä»matched start indexå¼€å§‹æŸ¥æ‰¾
			const startSearchFrom = sentenceInfo.matchedStartIndex !== -1 ? sentenceInfo.matchedStartIndex : 0

			// æˆ‘ä»¬æœŸæœ›æœ€åçš„éƒ¨åˆ†åœ¨å¥å­å¼€å§‹ä¹‹åçš„åˆç†ä½ç½®
			const expectedMinLength = sentenceInfo.normalizedSentence.length * 0.5
			const expectedMaxLength = sentenceInfo.normalizedSentence.length * 2

			// æœç´¢åœ¨åˆç†èŒƒå›´å†…çš„æœ€åä¸€ä¸ªå•è¯ç»„åˆ
			let lastPhraseIndex = -1
			let currentSearchPos = startSearchFrom

			while (true) {
				const nextOccurrence = normalizedTranscript.indexOf(lastWordsPhrase, currentSearchPos)
				if (nextOccurrence === -1) break

				// æ£€æŸ¥è¿™ä¸ªä½ç½®æ˜¯å¦åœ¨åˆç†çš„èŒƒå›´å†…
				const distance = nextOccurrence - startSearchFrom
				if (distance >= expectedMinLength && distance <= expectedMaxLength) {
					lastPhraseIndex = nextOccurrence
					break
				}

				currentSearchPos = nextOccurrence + 1
				if (currentSearchPos >= normalizedTranscript.length) break
			}

			if (lastPhraseIndex !== -1) {
				sentenceInfo.matchedEndIndex = lastPhraseIndex + lastWordsPhrase.length
				break
			}
		}
	}

	// ç¬¬äºŒé˜¶æ®µï¼šæ ¹æ®è¾¹ç•Œæå–å•è¯å¹¶åˆ›å»ºå¥å­
	for (const sentenceInfo of sentenceData) {
		if (sentenceInfo.processed) continue

		if (sentenceInfo.matchedStartIndex !== -1 && sentenceInfo.matchedEndIndex !== -1) {
			// æ‰¾åˆ°å¯¹åº”çš„å•è¯ç´¢å¼•
			let startWordIdx = -1
			let endWordIdx = -1
			let charCount = 0

			for (let i = 0; i < words.length; i++) {
				const prevCharCount = charCount
				charCount += words[i].word.length

				if (prevCharCount <= sentenceInfo.matchedStartIndex && charCount > sentenceInfo.matchedStartIndex && startWordIdx === -1) {
					startWordIdx = i
				}

				if (startWordIdx !== -1 && charCount >= sentenceInfo.matchedEndIndex) {
					endWordIdx = i
					break
				}
			}

			if (startWordIdx !== -1 && endWordIdx !== -1) {
				const sentenceWords = words.slice(startWordIdx, endWordIdx + 1)

				// è®¡ç®—ç›¸ä¼¼åº¦éªŒè¯åŒ¹é…çš„åˆç†æ€§
				const extractedText = sentenceWords.map((w) => w.word).join('')
				const normalizedExtracted = normalizeText(extractedText)
				const similarity = calculateSimilarity(normalizedExtracted, sentenceInfo.normalizedSentence)

				// ç›¸ä¼¼åº¦é˜ˆå€¼é™ä½åˆ°0.5ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ä½¿ç”¨äº†å¼€å§‹å’Œç»“å°¾é”šç‚¹
				if (similarity > 0.4) {
					result.push({
						words: sentenceWords,
						text: sentenceInfo.sentence,
						start: sentenceWords[0].start,
						end: sentenceWords[sentenceWords.length - 1].end,
					})
					sentenceInfo.processed = true
				}
			}
		}
	}

	// ç¬¬ä¸‰é˜¶æ®µï¼šä½¿ç”¨æ›´å®½æ¾çš„æ–¹æ³•å°è¯•åŒ¹é…å‰©ä½™çš„å¥å­
	const unprocessedSentences = sentenceData.filter((info) => !info.processed)

	if (unprocessedSentences.length > 0) {
		// å¯¹äºå‰©ä½™çš„å¥å­ï¼Œå°è¯•ä½¿ç”¨æ›´æ¨¡ç³Šçš„åŒ¹é…
		// æŒ‰å¥å­çš„èµ·å§‹ä½ç½®æ’åºç»“æœï¼Œä»¥ç¡®å®šç©ºç™½åŒºåŸŸ
		result.sort((a, b) => a.start - b.start)

		// æ‰¾å‡ºè½¬å½•æ–‡æœ¬ä¸­çš„ç©ºç™½åŒºåŸŸ
		const coveredRanges: { start: number; end: number }[] = result.map((item) => ({
			start: item.start,
			end: item.end,
		}))

		// åˆå¹¶é‡å çš„åŒºé—´
		coveredRanges.sort((a, b) => a.start - b.start)
		const mergedRanges: { start: number; end: number }[] = []

		for (const range of coveredRanges) {
			if (mergedRanges.length === 0) {
				mergedRanges.push(range)
				continue
			}

			const lastRange = mergedRanges[mergedRanges.length - 1]
			if (range.start <= lastRange.end) {
				// é‡å åŒºé—´ï¼Œåˆå¹¶
				lastRange.end = Math.max(lastRange.end, range.end)
			} else {
				// æ–°åŒºé—´
				mergedRanges.push(range)
			}
		}

		// æ‰¾å‡ºç©ºç™½åŒºåŸŸ
		const gaps: { start: number; end: number }[] = []
		let lastEnd = 0

		for (const range of mergedRanges) {
			if (range.start > lastEnd) {
				gaps.push({ start: lastEnd, end: range.start })
			}
			lastEnd = range.end
		}

		// å¦‚æœæœ€åä¸€ä¸ªåŒºé—´ç»“æŸåè¿˜æœ‰å•è¯ï¼Œæ·»åŠ æœ€åä¸€ä¸ªç©ºç™½åŒºåŸŸ
		if (lastEnd < words[words.length - 1].end) {
			gaps.push({ start: lastEnd, end: words[words.length - 1].end })
		}

		// å¯¹äºæ¯ä¸ªç©ºç™½åŒºåŸŸï¼Œå°è¯•å°†æœªå¤„ç†çš„å¥å­ä¸ä¹‹åŒ¹é…
		for (const gap of gaps) {
			// æ‰¾å‡ºè¿™ä¸ªç©ºç™½åŒºåŸŸå†…çš„å•è¯
			const gapWords = words.filter((word) => word.start >= gap.start && word.end <= gap.end)
			if (gapWords.length === 0) continue

			// å°è¯•åŒ¹é…æœªå¤„ç†çš„å¥å­
			for (const sentenceInfo of unprocessedSentences) {
				if (sentenceInfo.processed) continue

				const gapText = gapWords.map((w) => w.word).join('')
				const normalizedGapText = normalizeText(gapText)
				const similarity = calculateSimilarity(normalizedGapText, sentenceInfo.normalizedSentence)

				// ä½¿ç”¨æ›´ä½çš„ç›¸ä¼¼åº¦é˜ˆå€¼
				if (similarity > 0.35) {
					result.push({
						words: gapWords,
						text: sentenceInfo.sentence,
						start: gapWords[0].start,
						end: gapWords[gapWords.length - 1].end,
					})
					sentenceInfo.processed = true
					break // ä¸€ä¸ªç©ºç™½åŒºåŸŸåªåŒ¹é…ä¸€ä¸ªå¥å­
				}
			}
		}
	}

	// æŒ‰ç…§å•è¯çš„å¼€å§‹æ—¶é—´å¯¹ç»“æœæ’åº
	return result.sort((a, b) => a.start - b.start)
}

// è¾…åŠ©å‡½æ•°ï¼šæ‰¾åˆ°æœ€ä½³åŒ¹é…ä½ç½®
function findBestMatch(needle: string, haystack: string): number {
	// ç›´æ¥åŒ¹é…
	const directIndex = haystack.indexOf(needle)
	if (directIndex !== -1) {
		return directIndex
	}

	// å°è¯•æ¨¡ç³ŠåŒ¹é…
	// 1. ç§»é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ååŒ¹é…
	const cleanNeedle = needle.replace(/[.,\/#!$%\^&\*;:{}=\-_`~()]/g, '')
	const cleanHaystack = haystack.replace(/[.,\/#!$%\^&\*;:{}=\-_`~()]/g, '')
	const cleanIndex = cleanHaystack.indexOf(cleanNeedle)
	if (cleanIndex !== -1) {
		// æ‰¾åˆ°å¯¹åº”çš„åŸå§‹ç´¢å¼•
		let originalIndex = 0
		let cleanCounter = 0
		while (cleanCounter < cleanIndex && originalIndex < haystack.length) {
			if (!/[.,\/#!$%\^&\*;:{}=\-_`~()]/.test(haystack[originalIndex])) {
				cleanCounter++
			}
			originalIndex++
		}
		return originalIndex
	}

	// 2. å°è¯•åŒ¹é…å‰å‡ ä¸ªå•è¯
	const needleWords = needle.split(/\s+/)
	if (needleWords.length > 2) {
		// åŒ¹é…å‰3ä¸ªå•è¯ï¼Œå¦‚æœä¸è¡Œï¼Œå°è¯•å‰2ä¸ª
		const firstFewWords = needleWords.slice(0, 3).join(' ')
		const firstFewWordsIndex = haystack.indexOf(firstFewWords)
		if (firstFewWordsIndex !== -1) {
			return firstFewWordsIndex
		}

		// å°è¯•å‰2ä¸ªå•è¯
		const first2Words = needleWords.slice(0, 2).join(' ')
		return haystack.indexOf(first2Words)
	}

	return -1
}

// è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ (0-1)
function calculateSimilarity(str1: string, str2: string): number {
	if (!str1 || !str2) return 0

	// ä½¿ç”¨ Levenshtein è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
	const longerStr = str1.length > str2.length ? str1 : str2
	const shorterStr = str1.length > str2.length ? str2 : str1

	if (longerStr.length === 0) return 1.0

	// è®¡ç®—ç¼–è¾‘è·ç¦»
	const distance = levenshteinDistance(longerStr, shorterStr)

	// è®¡ç®—ç›¸ä¼¼åº¦
	return (longerStr.length - distance) / longerStr.length
}

// è®¡ç®— Levenshtein è·ç¦»
function levenshteinDistance(str1: string, str2: string): number {
	const m = str1.length
	const n = str2.length

	// åˆ›å»ºè·ç¦»çŸ©é˜µ
	const dp: number[][] = Array(m + 1)
		.fill(0)
		.map(() => Array(n + 1).fill(0))

	// åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
	for (let i = 0; i <= m; i++) dp[i][0] = i
	for (let j = 0; j <= n; j++) dp[0][j] = j

	// å¡«å……çŸ©é˜µ
	for (let i = 1; i <= m; i++) {
		for (let j = 1; j <= n; j++) {
			const cost = str1[i - 1] === str2[j - 1] ? 0 : 1
			dp[i][j] = Math.min(
				dp[i - 1][j] + 1, // åˆ é™¤
				dp[i][j - 1] + 1, // æ’å…¥
				dp[i - 1][j - 1] + cost, // æ›¿æ¢
			)
		}
	}

	return dp[m][n]
}

export async function splitTextToSentencesWithAI(sentence: string) {
	const WordsToSentencesSchema = z.object({
		sentences: z.array(z.string()),
	})

	const result = await deepSeek.generateObject({
		schema: WordsToSentencesSchema,
		system: `å°†è¾“å…¥æ–‡æœ¬åˆ†å‰²æˆæ›´çŸ­çš„å¥å­ï¼Œéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. ä¿æŒåŸæ–‡å†…å®¹å®Œæ•´ï¼Œä¸å¢å‡ã€ä¿®æ”¹æˆ–ç¿»è¯‘ä»»ä½•å†…å®¹
2. ä¸¥æ ¼æ§åˆ¶æ¯ä¸ªåˆ†å‰²åçš„å¥å­é•¿åº¦åœ¨40-60ä¸ªå­—ç¬¦ä¹‹é—´
3. åˆ†å‰²ä¼˜å…ˆçº§ï¼š
   - é¦–å…ˆåœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰è‡ªç„¶å¥æœ«å¤„åˆ†å‰²
   - å…¶æ¬¡åœ¨å¼•å·åã€é€—å·ã€åˆ†å·ã€å†’å·å¤„åˆ†å‰²
   - ç„¶ååœ¨è¿è¯ï¼ˆå¦‚and, but, or, because, about, whetherç­‰ï¼‰å¤„åˆ†å‰²
   - æœ€ååœ¨ä»‹è¯çŸ­è¯­ã€åè¯çŸ­è¯­æˆ–ä»å¥è¾¹ç•Œå¤„åˆ†å‰²
4. å¯¹äºå¸¦å¼•è¯­çš„é•¿å¥ï¼Œåœ¨å¼•è¯­ç»“æŸå¤„åˆ†å‰²
5. å¯¹äºæ²¡æœ‰æ˜æ˜¾åˆ†å‰²ç‚¹çš„é•¿å¥ï¼Œå¼ºåˆ¶åœ¨50-60å­—ç¬¦å¤„çš„å•è¯è¾¹ç•Œåˆ†å‰²
6. å¯¹äºè¶…è¿‡60å­—ç¬¦çš„å¥å­ï¼Œå¿…é¡»å†æ¬¡åˆ†å‰²ï¼Œä¸å…è®¸ä¾‹å¤–
7. ç¡®ä¿åˆ†å‰²åæ¯ä¸ªå¥å­ä¿æŒè¯­ä¹‰è¿è´¯æ€§
8. è¿”å›çš„æ‰€æœ‰å¥å­æ‹¼æ¥èµ·æ¥å¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´
9. æ¯æ¬¡åˆ†å‰²å‰æ£€æŸ¥å‰©ä½™æ–‡æœ¬é•¿åº¦ï¼Œé˜²æ­¢ç”Ÿæˆè¿‡çŸ­å¥å­ï¼ˆå°‘äº20å­—ç¬¦ï¼‰

éªŒè¯æ­¥éª¤ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰ï¼š
1. è®°å½•åŸå§‹è¾“å…¥æ–‡æœ¬çš„æ¯ä¸ªå­—ç¬¦
2. åˆ†å‰²åï¼Œæ£€æŸ¥æ¯ä¸ªå¥å­çš„å­—ç¬¦æ•°ï¼Œç¡®ä¿éƒ½åœ¨40-60èŒƒå›´å†…
3. å°†æ‰€æœ‰å¥å­æ‹¼æ¥å¹¶é€å­—ç¬¦æ¯”å¯¹åŸæ–‡ï¼Œç¡®ä¿100%åŒ¹é…
4. å¯¹äºä»»ä½•è¶…è¿‡60å­—ç¬¦çš„å¥å­ï¼Œç«‹å³é‡æ–°åˆ†å‰²
5. ç¡®ä¿æœ€åä¸€ä¸ªå¥å­ä¸ä¼šè¿‡çŸ­ï¼ˆå°‘äº20å­—ç¬¦ï¼‰`,
		prompt: sentence,
	})

	console.log('ğŸš€ ~ splitTextToSentencesWithAI ~ result:', result.sentences)

	return result.sentences
}
